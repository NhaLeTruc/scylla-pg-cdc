# Data Model: ScyllaDB to Postgres CDC Pipeline

**Date**: 2025-12-09
**Feature**: 001-scylla-pg-cdc

## Overview

This document defines the data entities, schemas, and relationships for the CDC pipeline infrastructure. The model includes metadata tables for checkpoint management, reconciliation tracking, and operational monitoring.

## Core Entities

### 1. Change Event (Kafka Message)

**Description**: Represents a single data modification event captured from ScyllaDB and flowing through Kafka to Postgres.

**Schema** (Avro):
```json
{
  "type": "record",
  "name": "ChangeEvent",
  "namespace": "com.scylla.cdc",
  "fields": [
    {
      "name": "correlation_id",
      "type": "string",
      "doc": "UUID for end-to-end tracing across systems"
    },
    {
      "name": "operation",
      "type": {
        "type": "enum",
        "name": "OperationType",
        "symbols": ["INSERT", "UPDATE", "DELETE", "TRUNCATE"]
      },
      "doc": "Type of change operation"
    },
    {
      "name": "table_name",
      "type": "string",
      "doc": "Fully qualified table name (keyspace.table)"
    },
    {
      "name": "primary_key",
      "type": {
        "type": "map",
        "values": ["null", "string", "long", "double", "boolean"]
      },
      "doc": "Primary key column(s) and values"
    },
    {
      "name": "before",
      "type": ["null", {
        "type": "map",
        "values": ["null", "string", "long", "double", "boolean", "bytes"]
      }],
      "default": null,
      "doc": "Column values before change (for UPDATE/DELETE)"
    },
    {
      "name": "after",
      "type": ["null", {
        "type": "map",
        "values": ["null", "string", "long", "double", "boolean", "bytes"]
      }],
      "default": null,
      "doc": "Column values after change (for INSERT/UPDATE)"
    },
    {
      "name": "commit_timestamp",
      "type": "long",
      "doc": "ScyllaDB commit timestamp (microseconds since epoch)"
    },
    {
      "name": "captured_at",
      "type": "long",
      "doc": "Timestamp when connector captured event (milliseconds since epoch)"
    },
    {
      "name": "schema_version",
      "type": "int",
      "doc": "Schema Registry version number for this table"
    }
  ]
}
```

**Validation Rules**:
- `correlation_id` must be valid UUID v4
- `operation` must be one of enum values
- `table_name` must match pattern `<keyspace>.<table>`
- `primary_key` must not be null or empty
- For INSERT: `after` must be present, `before` is null
- For UPDATE: both `before` and `after` must be present
- For DELETE: `before` must be present, `after` is null
- `commit_timestamp` must be <= `captured_at` (allowing for clock skew)

**State Transitions**:
1. Event generated by ScyllaDB CDC mechanism
2. Captured by Scylla Source Connector
3. Serialized to Avro with schema validation
4. Published to Kafka topic (partitioned by table + primary key hash)
5. Consumed by Postgres Sink Connector
6. Transformed and applied to Postgres table

### 2. Checkpoint (Kafka Connect Internal)

**Description**: Tracks connector processing progress to enable exactly-once semantics and recovery.

**Schema** (Kafka Connect internal format):
```json
{
  "connector_name": "scylla-source-users",
  "partition": {
    "table": "keyspace.users",
    "scylla_partition": "0"
  },
  "offset": {
    "stream_id": "8f3a2c1b-...",
    "window_start_timestamp": 1702137600000000,
    "window_end_timestamp": 1702137660000000
  }
}
```

**Storage**: Kafka internal topic `__connect-offsets`

**Validation Rules**:
- Offsets atomically committed with Kafka transactions
- Checkpoint updated only after successful sink write
- Per-table, per-partition granularity

**Usage**:
- On connector restart, resume from last committed offset
- Enables exactly-once processing by preventing duplicate reads

### 3. Schema Version (Schema Registry)

**Description**: Captures table schema at a point in time with compatibility validation.

**Schema** (Schema Registry metadata):
```json
{
  "subject": "keyspace.users-value",
  "version": 3,
  "id": 42,
  "schema": "{\"type\":\"record\",\"name\":\"users\",\"fields\":[...]}",
  "compatibility": "BACKWARD",
  "registered_at": "2025-12-09T10:30:00Z"
}
```

**Attributes**:
- **subject**: `<keyspace>.<table>-value` (Kafka topic + value suffix)
- **version**: Monotonically increasing integer
- **id**: Global unique schema ID
- **schema**: Avro schema JSON string
- **compatibility**: BACKWARD | FORWARD | FULL | NONE
- **registered_at**: Timestamp of schema registration

**Validation Rules**:
- New schema must pass compatibility check with previous version
- BACKWARD compatibility: new schema can read old data (allows adding fields with defaults, removing fields)
- Schema evolution automatically handled by connectors

**Relationships**:
- Each ChangeEvent references schema_version (id field)
- Sink connector fetches schema by id to deserialize

### 4. Dead Letter Message (Kafka Topic)

**Description**: Failed events that exceeded retry threshold, preserved for investigation.

**Schema**:
```json
{
  "original_topic": "keyspace.users",
  "partition": 3,
  "offset": 12345,
  "key": "{\"user_id\": 42}",
  "value": "{... original event ...}",
  "exception": {
    "class": "org.apache.kafka.connect.errors.DataException",
    "message": "Schema incompatibility: field 'email' type mismatch",
    "stacktrace": "..."
  },
  "retry_count": 10,
  "first_failed_at": "2025-12-09T10:00:00Z",
  "last_failed_at": "2025-12-09T10:05:00Z"
}
```

**Storage**: Kafka topics `dlq-scylla-source`, `dlq-postgres-sink`

**Attributes**:
- **original_topic**: Source topic where event originated
- **partition/offset**: Original location for replay
- **key/value**: Original event payload
- **exception**: Error details for debugging
- **retry_count**: Number of retry attempts before DLQ
- **first_failed_at**: Timestamp of initial failure
- **last_failed_at**: Timestamp of final retry

**Operational Procedures**:
1. Monitor DLQ message count (alert if > 0)
2. Investigate failure root cause
3. Fix underlying issue (schema, data, configuration)
4. Replay messages from DLQ to original topic (if resolvable)
5. Or manually correct data in Postgres (if source issue)

## Infrastructure Metadata Tables (Postgres)

### 5. Reconciliation Checkpoints

**Description**: Tracks reconciliation progress to enable resumable reconciliation.

**Schema** (PostgreSQL):
```sql
CREATE TABLE cdc_metadata.reconciliation_checkpoints (
    table_name VARCHAR(255) PRIMARY KEY,
    last_reconciled_key TEXT NOT NULL,
    reconciliation_started_at TIMESTAMP NOT NULL,
    reconciliation_completed_at TIMESTAMP,
    total_rows_checked BIGINT DEFAULT 0,
    discrepancies_found INT DEFAULT 0,
    status VARCHAR(20) NOT NULL CHECK (status IN ('running', 'completed', 'failed')),
    error_message TEXT
);

CREATE INDEX idx_reconciliation_status ON cdc_metadata.reconciliation_checkpoints(status);
```

**Attributes**:
- **table_name**: Fully qualified table name (keyspace.table)
- **last_reconciled_key**: Serialized primary key of last checked row
- **reconciliation_started_at**: When reconciliation began
- **reconciliation_completed_at**: When reconciliation finished (null if running)
- **total_rows_checked**: Progress counter
- **discrepancies_found**: Count of mismatches found
- **status**: Current reconciliation state
- **error_message**: If status=failed, describes error

**Usage**:
- Before starting reconciliation, check for existing running reconciliation
- Resume from `last_reconciled_key` if previous run interrupted
- Update checkpoint every 10k rows processed
- Mark completed when entire table scanned

### 6. Reconciliation Log

**Description**: Records discovered discrepancies between ScyllaDB and Postgres.

**Schema** (PostgreSQL):
```sql
CREATE TABLE cdc_metadata.reconciliation_log (
    id BIGSERIAL PRIMARY KEY,
    table_name VARCHAR(255) NOT NULL,
    primary_key_json JSONB NOT NULL,
    discrepancy_type VARCHAR(50) NOT NULL CHECK (discrepancy_type IN ('missing', 'mismatch', 'extra')),
    scylla_value_json JSONB,
    postgres_value_json JSONB,
    discovered_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    resolved BOOLEAN NOT NULL DEFAULT FALSE,
    resolved_at TIMESTAMP,
    resolution_method VARCHAR(100)
);

CREATE INDEX idx_reconciliation_table ON cdc_metadata.reconciliation_log(table_name);
CREATE INDEX idx_reconciliation_resolved ON cdc_metadata.reconciliation_log(resolved);
CREATE INDEX idx_reconciliation_discovered ON cdc_metadata.reconciliation_log(discovered_at);
```

**Attributes**:
- **id**: Auto-incrementing primary key
- **table_name**: Table where discrepancy found
- **primary_key_json**: Primary key identifying the row
- **discrepancy_type**:
  - `missing`: Row exists in ScyllaDB but not Postgres
  - `mismatch`: Row exists in both but column values differ
  - `extra`: Row exists in Postgres but not ScyllaDB (deleted?)
- **scylla_value_json**: Full row from ScyllaDB (null if extra)
- **postgres_value_json**: Full row from Postgres (null if missing)
- **discovered_at**: When discrepancy detected
- **resolved**: Whether discrepancy has been fixed
- **resolved_at**: When discrepancy was resolved
- **resolution_method**: How it was resolved (e.g., "auto_repair", "manual_fix")

**Usage**:
- Reconciliation script inserts row for each discrepancy
- Repair script queries unresolved discrepancies and fixes them
- Monitoring dashboard shows discrepancy trends over time

### 7. Reconciliation Repairs

**Description**: Audit trail of reconciliation repair actions.

**Schema** (PostgreSQL):
```sql
CREATE TABLE cdc_metadata.reconciliation_repairs (
    id BIGSERIAL PRIMARY KEY,
    reconciliation_log_id BIGINT REFERENCES cdc_metadata.reconciliation_log(id),
    table_name VARCHAR(255) NOT NULL,
    primary_key_json JSONB NOT NULL,
    repair_action VARCHAR(50) NOT NULL CHECK (repair_action IN ('insert', 'update', 'delete', 'skip')),
    repair_query TEXT,
    executed_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    success BOOLEAN NOT NULL,
    error_message TEXT
);

CREATE INDEX idx_repairs_table ON cdc_metadata.reconciliation_repairs(table_name);
CREATE INDEX idx_repairs_success ON cdc_metadata.reconciliation_repairs(success);
CREATE INDEX idx_repairs_executed ON cdc_metadata.reconciliation_repairs(executed_at);
```

**Attributes**:
- **id**: Auto-incrementing primary key
- **reconciliation_log_id**: Link to discrepancy record
- **table_name**: Table being repaired
- **primary_key_json**: Row identifier
- **repair_action**: Type of repair SQL executed
- **repair_query**: Actual SQL query run (for audit)
- **executed_at**: When repair attempted
- **success**: Whether repair succeeded
- **error_message**: If failed, describes error

**Usage**:
- Every repair action logged for audit trail
- Failed repairs trigger alerts for manual intervention
- Dashboard shows repair success rate

## Kafka Topics

### Naming Convention

**Pattern**: `<source_keyspace>.<source_table>`

**Examples**:
- `ecommerce.users` (for keyspace=ecommerce, table=users)
- `analytics.events` (for keyspace=analytics, table=events)

### Topic Configuration

**Production Settings**:
```properties
# Partitions: 2x number of Scylla partitions or 8 (whichever is greater)
num.partitions=8

# Replication: 3 for durability
replication.factor=3

# Retention: 24 hours minimum (longer for recovery window)
retention.ms=86400000

# Cleanup policy: delete old segments
cleanup.policy=delete

# Compression: Snappy for balance of speed/size
compression.type=snappy

# Min in-sync replicas: 2 for durability
min.insync.replicas=2
```

**Development Settings**:
```properties
num.partitions=4
replication.factor=1
retention.ms=3600000
min.insync.replicas=1
```

### Partitioning Strategy

**Key**: Hash of primary key columns
**Rationale**: Ensures all events for same row go to same partition, maintaining per-key ordering

**Example**:
```python
# Pseudo-code for partition assignment
partition = hash(primary_key) % num_partitions
```

## Entity Relationships

```
┌─────────────┐
│  ScyllaDB   │ (source)
│  CDC Logs   │
└──────┬──────┘
       │
       │ [captured by]
       ▼
┌─────────────────────┐
│ Scylla Source       │
│ Connector           │ ──┐
└──────┬──────────────┘   │
       │                  │ [stores offset]
       │ [produces]       ▼
       │            ┌──────────────┐
       │            │ __connect-   │
       │            │  offsets     │ (Kafka internal topic)
       │            └──────────────┘
       ▼
┌─────────────────────┐
│ Kafka Topic         │
│ (Change Events)     │ ◄──┐
└──────┬──────────────┘    │
       │                   │ [references schema]
       │                   │
       │                   │
       │ [consumed by]     ▼
       │            ┌──────────────┐
       │            │ Schema       │
       │            │ Registry     │
       │            └──────────────┘
       ▼
┌─────────────────────┐
│ Postgres JDBC Sink  │
│ Connector           │
└──────┬──────────────┘
       │
       │ [writes to]
       ▼
┌─────────────┐       ┌─────────────────────┐
│  Postgres   │       │ cdc_metadata schema │
│  Warehouse  │       ├─────────────────────┤
│  (target)   │       │ reconciliation_     │
└─────────────┘       │  checkpoints        │
                      │ reconciliation_log  │
                      │ reconciliation_     │
                      │  repairs            │
                      └─────────────────────┘
                               ▲
                               │
                               │ [read/write by]
                               │
                      ┌────────┴────────┐
                      │ Reconciliation  │
                      │ Script          │
                      └─────────────────┘
```

## Data Flow Sequence

### Normal Flow (Happy Path)

1. **Change Occurs in ScyllaDB**:
   - User application executes INSERT/UPDATE/DELETE
   - ScyllaDB writes change to CDC log table

2. **Scylla Source Connector Polls**:
   - Connector reads from CDC log (poll interval: 100ms)
   - Extracts change event details
   - Generates correlation ID
   - Serializes to Avro using schema from Schema Registry
   - Publishes to Kafka topic (transactional)
   - Commits offset to `__connect-offsets` (atomic with publish)

3. **Event Stored in Kafka**:
   - Message replicated across brokers (replication factor)
   - Available for consumption with read_committed isolation

4. **Postgres Sink Connector Consumes**:
   - Connector polls Kafka topic (fetch.min.bytes batch)
   - Deserializes Avro using Schema Registry
   - Accumulates batch (up to batch.size messages)
   - Generates SQL (INSERT ON CONFLICT UPDATE)
   - Executes transaction against Postgres
   - Commits Kafka consumer offset (after successful DB commit)

5. **Event Applied to Postgres**:
   - Row inserted/updated/deleted in target table
   - Query completes, transaction committed

### Failure Scenarios

**Scenario A: Network Partition Between Scylla and Kafka Connect**
- Source connector poll fails with timeout exception
- Retry with exponential backoff (initial: 1s, max: 30s)
- No events lost (CDC log persisted in ScyllaDB)
- When network restored, resume from last committed offset

**Scenario B: Schema Incompatibility**
- Source connector reads event with new column
- Attempts to register schema with Schema Registry
- Compatibility check fails (e.g., removed required field)
- Event sent to `dlq-scylla-source` topic
- Alert triggered for manual intervention
- Pipeline continues processing other events

**Scenario C: Postgres Primary Key Violation**
- Sink connector attempts INSERT with duplicate key
- UPSERT mode: automatically converts to UPDATE
- No error, idempotent operation succeeds
- Offset committed

**Scenario D: Connector Crash Mid-Processing**
- Worker process killed (OOM, SIGKILL, etc.)
- Kafka rebalances tasks to remaining workers
- New worker reads last committed offset from `__connect-offsets`
- Resumes processing from checkpoint
- Some events may be reprocessed (idempotent sinks handle duplicates)

## Schema Evolution Examples

### Example 1: Add Column with Default

**ScyllaDB Change**:
```sql
ALTER TABLE keyspace.users ADD COLUMN phone_number TEXT DEFAULT '';
```

**Impact**:
- Scylla Source Connector detects new column in CDC log
- Registers new schema version with Schema Registry (v2)
- Schema Registry validates BACKWARD compatibility: ✅ (new field has default)
- Postgres Sink Connector automatically adds column via `auto.evolve=true`
- No pipeline downtime

**Avro Schema Evolution**:
```json
// Version 1
{"fields": [{"name": "user_id", "type": "long"}, {"name": "email", "type": "string"}]}

// Version 2 (backward compatible)
{"fields": [
  {"name": "user_id", "type": "long"},
  {"name": "email", "type": "string"},
  {"name": "phone_number", "type": "string", "default": ""}
]}
```

### Example 2: Remove Column

**ScyllaDB Change**:
```sql
ALTER TABLE keyspace.users DROP COLUMN middle_name;
```

**Impact**:
- Scylla Source Connector detects missing column
- Registers new schema version omitting field
- Schema Registry validates BACKWARD compatibility: ✅ (old consumers can ignore missing field)
- Postgres Sink Connector ignores missing field (column remains in Postgres for historical data)
- Or optionally: run manual migration to drop column in Postgres

### Example 3: Incompatible Change (Requires Manual Intervention)

**ScyllaDB Change**:
```sql
ALTER TABLE keyspace.users ALTER COLUMN age TYPE TEXT;  -- INT → TEXT
```

**Impact**:
- Scylla Source Connector attempts to register new schema
- Schema Registry compatibility check: ❌ FAILS (type change breaks backward compatibility)
- Connector logs error and sends event to DLQ
- Alert triggered: "Schema evolution blocked for keyspace.users"
- Manual steps:
  1. Pause connector
  2. Create new Kafka topic with v2 schema
  3. Update sink connector to consume from new topic
  4. Resume source connector with new topic
  5. Run data migration script for historical data

## Monitoring Queries

### Check Replication Lag
```sql
-- Postgres: Compare row counts
SELECT 'scylla' AS source, COUNT(*) FROM keyspace.users_replica
UNION ALL
SELECT 'postgres' AS source, COUNT(*) FROM keyspace.users;
```

### Check Active Reconciliations
```sql
SELECT table_name,
       reconciliation_started_at,
       total_rows_checked,
       discrepancies_found,
       EXTRACT(EPOCH FROM (NOW() - reconciliation_started_at))/60 AS minutes_running
FROM cdc_metadata.reconciliation_checkpoints
WHERE status = 'running';
```

### Check Unresolved Discrepancies
```sql
SELECT table_name,
       discrepancy_type,
       COUNT(*) AS count
FROM cdc_metadata.reconciliation_log
WHERE NOT resolved
GROUP BY table_name, discrepancy_type
ORDER BY count DESC;
```

### Check Repair Success Rate
```sql
SELECT table_name,
       COUNT(*) AS total_repairs,
       SUM(CASE WHEN success THEN 1 ELSE 0 END) AS successful,
       ROUND(100.0 * SUM(CASE WHEN success THEN 1 ELSE 0 END) / COUNT(*), 2) AS success_rate
FROM cdc_metadata.reconciliation_repairs
WHERE executed_at > NOW() - INTERVAL '24 hours'
GROUP BY table_name;
```
