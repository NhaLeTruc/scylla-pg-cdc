# ==============================================================================
# Kafka Connect Distributed Mode Configuration
# ScyllaDB to Postgres CDC Pipeline
# ==============================================================================
# This file configures Kafka Connect in distributed mode with enhanced logging
# for schema evolution and change data capture operations.
# ==============================================================================

# ------------------------------------------------------------------------------
# Bootstrap Configuration
# ------------------------------------------------------------------------------
bootstrap.servers=kafka:9092
group.id=scylla-pg-cdc-connect-cluster

# ------------------------------------------------------------------------------
# Offset, Config, and Status Topic Configuration
# ------------------------------------------------------------------------------
# These topics store connector state, configuration, and offsets
offset.storage.topic=connect-offsets
offset.storage.replication.factor=1
offset.storage.partitions=25

config.storage.topic=connect-configs
config.storage.replication.factor=1

status.storage.topic=connect-status
status.storage.replication.factor=1
status.storage.partitions=5

# Increase timeouts for stability
offset.flush.interval.ms=10000
offset.flush.timeout.ms=5000

# ------------------------------------------------------------------------------
# Converter Configuration
# ------------------------------------------------------------------------------
# Use Avro with Schema Registry for schema evolution support
key.converter=io.confluent.connect.avro.AvroConverter
key.converter.schema.registry.url=http://schema-registry:8081
key.converter.schemas.enable=true

value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=http://schema-registry:8081
value.converter.schemas.enable=true

# Internal converters for Connect framework
internal.key.converter=org.apache.kafka.connect.json.JsonConverter
internal.value.converter=org.apache.kafka.connect.json.JsonConverter
internal.key.converter.schemas.enable=false
internal.value.converter.schemas.enable=false

# Header converter
header.converter=org.apache.kafka.connect.storage.StringConverter

# ------------------------------------------------------------------------------
# Schema Registry Configuration
# ------------------------------------------------------------------------------
# Enhanced schema registry settings for evolution tracking
schema.registry.url=http://schema-registry:8081

# Schema caching settings
schema.registry.cache.capacity=1000
schema.registry.cache.expiry.secs=3600

# Schema compatibility settings
# BACKWARD: New schema can read old data (default, safest)
# FORWARD: Old schema can read new data
# FULL: Both backward and forward compatible
# NONE: No compatibility checking
value.converter.schema.registry.basic.auth.credentials.source=USER_INFO
value.converter.schema.registry.basic.auth.user.info=${SCHEMA_REGISTRY_USER}:${SCHEMA_REGISTRY_PASSWORD}

# ------------------------------------------------------------------------------
# REST API Configuration
# ------------------------------------------------------------------------------
rest.port=8083
rest.advertised.host.name=kafka-connect
rest.advertised.port=8083

# CORS configuration for UI access
rest.extension.classes=org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension

# ------------------------------------------------------------------------------
# Plugin Configuration
# ------------------------------------------------------------------------------
plugin.path=/usr/share/java,/usr/share/confluent-hub-components,/etc/kafka-connect/plugins

# ------------------------------------------------------------------------------
# Producer Configuration
# ------------------------------------------------------------------------------
# Optimized for exactly-once semantics and performance
producer.compression.type=snappy
producer.batch.size=16384
producer.linger.ms=10
producer.max.in.flight.requests.per.connection=5
producer.acks=all
producer.enable.idempotence=true
producer.max.request.size=1048576

# Retry configuration
producer.retries=2147483647
producer.retry.backoff.ms=100

# ------------------------------------------------------------------------------
# Consumer Configuration
# ------------------------------------------------------------------------------
consumer.fetch.min.bytes=1
consumer.fetch.max.wait.ms=500
consumer.max.partition.fetch.bytes=1048576
consumer.session.timeout.ms=30000
consumer.heartbeat.interval.ms=3000
consumer.max.poll.interval.ms=300000
consumer.auto.offset.reset=earliest

# ------------------------------------------------------------------------------
# Task Configuration
# ------------------------------------------------------------------------------
task.shutdown.graceful.timeout.ms=30000
scheduled.rebalance.max.delay.ms=300000

# ------------------------------------------------------------------------------
# Metrics and Monitoring
# ------------------------------------------------------------------------------
# JMX metrics
metrics.recording.level=INFO
metrics.sample.window.ms=30000
metrics.num.samples=2

# Prometheus metrics reporter
metric.reporters=io.confluent.telemetry.reporter.TelemetryReporter

# ------------------------------------------------------------------------------
# Logging Configuration
# ------------------------------------------------------------------------------
# Enhanced logging for schema evolution and CDC operations

# Root logger level
log4j.rootLogger=INFO, stdout, file

# Console appender
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=[%d] %p %c{1} - %m%n

# File appender
log4j.appender.file=org.apache.log4j.RollingFileAppender
log4j.appender.file.File=/var/log/kafka-connect/connect.log
log4j.appender.file.MaxFileSize=100MB
log4j.appender.file.MaxBackupIndex=10
log4j.appender.file.layout=org.apache.log4j.PatternLayout
log4j.appender.file.layout.ConversionPattern=[%d] %p %c{1} - %m%n

# ------------------------------------------------------------------------------
# Schema Evolution Logging
# ------------------------------------------------------------------------------
# CRITICAL: These loggers track schema changes, compatibility checks, and evolution

# Schema Registry client - logs all schema registration and retrieval
log4j.logger.io.confluent.kafka.schemaregistry=INFO, schema_registry
log4j.logger.io.confluent.kafka.schemaregistry.client=DEBUG, schema_registry
log4j.additivity.io.confluent.kafka.schemaregistry=false

# Schema Registry file appender
log4j.appender.schema_registry=org.apache.log4j.RollingFileAppender
log4j.appender.schema_registry.File=/var/log/kafka-connect/schema-registry.log
log4j.appender.schema_registry.MaxFileSize=50MB
log4j.appender.schema_registry.MaxBackupIndex=5
log4j.appender.schema_registry.layout=org.apache.log4j.PatternLayout
log4j.appender.schema_registry.layout.ConversionPattern=[%d{ISO8601}] %p [%c] %m%n

# Avro converter - logs serialization/deserialization and schema evolution
log4j.logger.io.confluent.connect.avro=DEBUG, schema_evolution
log4j.additivity.io.confluent.connect.avro=false

# Schema evolution file appender
log4j.appender.schema_evolution=org.apache.log4j.RollingFileAppender
log4j.appender.schema_evolution.File=/var/log/kafka-connect/schema-evolution.log
log4j.appender.schema_evolution.MaxFileSize=50MB
log4j.appender.schema_evolution.MaxBackupIndex=10
log4j.appender.schema_evolution.layout=org.apache.log4j.PatternLayout
log4j.appender.schema_evolution.layout.ConversionPattern=[%d{ISO8601}] %p [%c{2}] [schema-change] %m%n

# Schema compatibility checking
log4j.logger.io.confluent.kafka.schemaregistry.avro.AvroCompatibilityChecker=DEBUG, schema_evolution

# ------------------------------------------------------------------------------
# CDC and Connector Logging
# ------------------------------------------------------------------------------

# ScyllaDB source connector - logs CDC operations
log4j.logger.com.scylladb.cdc=INFO, cdc
log4j.logger.io.debezium=INFO, cdc
log4j.additivity.com.scylladb.cdc=false
log4j.additivity.io.debezium=false

# CDC file appender
log4j.appender.cdc=org.apache.log4j.RollingFileAppender
log4j.appender.cdc.File=/var/log/kafka-connect/cdc.log
log4j.appender.cdc.MaxFileSize=100MB
log4j.appender.cdc.MaxBackupIndex=10
log4j.appender.cdc.layout=org.apache.log4j.PatternLayout
log4j.appender.cdc.layout.ConversionPattern=[%d{ISO8601}] %p [%c{2}] [cdc] %m%n

# PostgreSQL sink connector
log4j.logger.io.confluent.connect.jdbc=INFO, sink
log4j.additivity.io.confluent.connect.jdbc=false

# Sink connector file appender
log4j.appender.sink=org.apache.log4j.RollingFileAppender
log4j.appender.sink.File=/var/log/kafka-connect/sink.log
log4j.appender.sink.MaxFileSize=100MB
log4j.appender.sink.MaxBackupIndex=10
log4j.appender.sink.layout=org.apache.log4j.PatternLayout
log4j.appender.sink.layout.ConversionPattern=[%d{ISO8601}] %p [%c{2}] [sink] %m%n

# ------------------------------------------------------------------------------
# Error and Dead Letter Queue Logging
# ------------------------------------------------------------------------------

# DLQ errors - critical for tracking schema incompatibilities
log4j.logger.org.apache.kafka.connect.runtime.errors=WARN, dlq
log4j.additivity.org.apache.kafka.connect.runtime.errors=false

# DLQ file appender
log4j.appender.dlq=org.apache.log4j.RollingFileAppender
log4j.appender.dlq.File=/var/log/kafka-connect/dlq-errors.log
log4j.appender.dlq.MaxFileSize=50MB
log4j.appender.dlq.MaxBackupIndex=10
log4j.appender.dlq.layout=org.apache.log4j.PatternLayout
log4j.appender.dlq.layout.ConversionPattern=[%d{ISO8601}] %p [%c{2}] [DLQ] %m%n

# Transformation errors
log4j.logger.org.apache.kafka.connect.transforms=INFO, transforms
log4j.additivity.org.apache.kafka.connect.transforms=false

# Transforms file appender
log4j.appender.transforms=org.apache.log4j.RollingFileAppender
log4j.appender.transforms.File=/var/log/kafka-connect/transforms.log
log4j.appender.transforms.MaxFileSize=25MB
log4j.appender.transforms.MaxBackupIndex=5
log4j.appender.transforms.layout=org.apache.log4j.PatternLayout
log4j.appender.transforms.layout.ConversionPattern=[%d{ISO8601}] %p [%c{2}] [transform] %m%n

# ------------------------------------------------------------------------------
# Connect Framework Logging
# ------------------------------------------------------------------------------

# Worker coordination
log4j.logger.org.apache.kafka.connect.runtime.WorkerCoordinator=INFO, stdout
log4j.logger.org.apache.kafka.connect.runtime.distributed=INFO, stdout

# Task execution
log4j.logger.org.apache.kafka.connect.runtime.Worker=INFO, stdout
log4j.logger.org.apache.kafka.connect.runtime.WorkerTask=INFO, stdout
log4j.logger.org.apache.kafka.connect.runtime.WorkerSourceTask=INFO, stdout
log4j.logger.org.apache.kafka.connect.runtime.WorkerSinkTask=INFO, stdout

# REST API
log4j.logger.org.apache.kafka.connect.rest=INFO, stdout

# Storage
log4j.logger.org.apache.kafka.connect.storage=INFO, stdout

# Rebalancing
log4j.logger.org.apache.kafka.connect.runtime.rebalance=INFO, stdout

# ------------------------------------------------------------------------------
# Performance and Troubleshooting Loggers
# ------------------------------------------------------------------------------

# Network and IO
log4j.logger.org.apache.kafka.clients=WARN, stdout
log4j.logger.org.apache.kafka.clients.NetworkClient=WARN, stdout

# Admin client for connector management
log4j.logger.org.apache.kafka.clients.admin=INFO, stdout

# Consumer and Producer
log4j.logger.org.apache.kafka.clients.consumer=INFO, stdout
log4j.logger.org.apache.kafka.clients.producer=INFO, stdout

# Metadata
log4j.logger.org.apache.kafka.common.utils.AppInfoParser=WARN, stdout

# ==============================================================================
# Schema Evolution Guidelines
# ==============================================================================
#
# BACKWARD compatible changes (old consumers can read new data):
#   - Add optional fields with defaults
#   - Remove fields
#
# FORWARD compatible changes (new consumers can read old data):
#   - Add fields
#   - Remove optional fields
#
# FULL compatible changes (both directions):
#   - Add optional fields with defaults
#
# Breaking changes (require new topic or version):
#   - Change field type
#   - Rename field without alias
#   - Remove required field
#   - Add required field without default
#
# Monitor these log files for schema evolution events:
#   - /var/log/kafka-connect/schema-evolution.log (schema changes)
#   - /var/log/kafka-connect/schema-registry.log (registry operations)
#   - /var/log/kafka-connect/dlq-errors.log (incompatible changes)
#
# ==============================================================================
